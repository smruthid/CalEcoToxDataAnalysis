{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7567275-b6ab-4da8-a3e1-6bc3d858ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, mean_squared_error, r2_score\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb61fba2-b1b4-462c-8917-b461fb81b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(input_file='Data/ToxicityPhylogeneticChemical.csv'):    \n",
    "    all_data = pd.read_csv(input_file)\n",
    "    \n",
    "    # Function to extract just the number\n",
    "    def extract_number(endpoint_str):\n",
    "        if pd.isna(endpoint_str):\n",
    "            return None\n",
    "        # Remove everything except numbers and decimal points\n",
    "        match = re.search(r'(\\d+\\.?\\d*)', str(endpoint_str))\n",
    "        return float(match.group(1)) if match else None\n",
    "\n",
    "    # Function to extract units (mg/l or mg/kg)\n",
    "    def extract_units(endpoint_str):\n",
    "        if pd.isna(endpoint_str):\n",
    "            return None\n",
    "        endpoint_str = str(endpoint_str).lower()\n",
    "        if 'mg/l' in endpoint_str:\n",
    "            return 'mg/l'\n",
    "        elif 'mg/kg' in endpoint_str:\n",
    "            return 'mg/kg'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Apply the functions\n",
    "    all_data['Endpoint Val'] = all_data['Endpoint Value'].apply(extract_number)\n",
    "    all_data['Endpoint Units'] = all_data['Endpoint Value'].apply(extract_units)\n",
    "    all_data = all_data.drop(columns=['Endpoint Value'])\n",
    "\n",
    "    # Filter and clip endpoint values\n",
    "    all_data = all_data[all_data['Endpoint Val'] > 0.005]\n",
    "    all_data['Endpoint Val'] = all_data['Endpoint Val'].clip(lower=0.01, upper=5000)\n",
    "\n",
    "    # Hierarchical encoding of phylogenetic data\n",
    "    class_features = pd.get_dummies(all_data['class'], prefix='class')\n",
    "    order_features = pd.get_dummies(all_data['order'], prefix='order') \n",
    "    family_features = pd.get_dummies(all_data['family'], prefix='family')\n",
    "    genus_features = pd.get_dummies(all_data['genus'], prefix='genus')\n",
    "\n",
    "    hierarchical_features = pd.concat([\n",
    "        class_features,\n",
    "        order_features, \n",
    "        family_features,\n",
    "        genus_features\n",
    "    ], axis=1)\n",
    "\n",
    "    all_data = pd.concat([all_data, hierarchical_features], axis=1)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = ['Unnamed: 0', 'Common Name', 'Range', 'Sex', 'Chemical', \n",
    "                      'ChemicalName', 'Sample Size', 'Tox Exposure', 'species', \n",
    "                      'class', 'order', 'family', 'genus']\n",
    "    # Only drop columns that exist\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in all_data.columns]\n",
    "    all_data = all_data.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Normalizing Chemical Data\n",
    "    all_data['Lipinski_Violations'] = (\n",
    "        (all_data['MolecularWeight'] > 500).astype(int) +\n",
    "        (all_data['XLogP'] > 5).astype(int) +\n",
    "        (all_data['HBondDonorCount'] > 5).astype(int) +\n",
    "        (all_data['HBondAcceptorCount'] > 10).astype(int)\n",
    "    )\n",
    "\n",
    "    all_data['Total_HBonds'] = all_data['HBondDonorCount'] + all_data['HBondAcceptorCount']\n",
    "    \n",
    "    # Standardize molecular features\n",
    "    scaler = StandardScaler()\n",
    "    all_data['MolecularWeight'] = scaler.fit_transform(all_data[['MolecularWeight']])\n",
    "    all_data['TPSA'] = StandardScaler().fit_transform(all_data[['TPSA']])\n",
    "    all_data['XLogP'] = all_data['XLogP'].fillna(all_data['XLogP'].median())\n",
    "\n",
    "    # Handling Tox Exposure Duration\n",
    "    arr = ['Single dose','single', 'single dose', 'Single exposure']\n",
    "    all_data['Tox Exposure Duration'] = all_data['Tox Exposure Duration'].replace(arr, 0.1)\n",
    "    all_data['Tox Exposure Duration'] = all_data['Tox Exposure Duration'].fillna(0.1)\n",
    "    all_data['Tox Exposure Duration'] = all_data['Tox Exposure Duration'].astype('float64')\n",
    "\n",
    "    # Handling Tox Exposure Technique\n",
    "    all_data['Tox Exposure Technique'] = all_data['Tox Exposure Technique'].fillna('unknown')\n",
    "    technique_features = pd.get_dummies(all_data['Tox Exposure Technique'], prefix='technique')\n",
    "    all_data = pd.concat([all_data, technique_features], axis=1)\n",
    "    all_data = all_data.drop(columns=['Tox Exposure Technique'])\n",
    "\n",
    "    # Encoding Life Cycle Stage\n",
    "    lifecycle_features = pd.get_dummies(all_data['Life Cycle Stage'], prefix='stage')\n",
    "    all_data = pd.concat([all_data, lifecycle_features], axis=1)\n",
    "    all_data = all_data.drop(columns=['Life Cycle Stage'])\n",
    "\n",
    "    # Log transforming endpoint value\n",
    "    all_data['log_endpoint_val'] = np.log(all_data['Endpoint Val'])\n",
    "    all_data = all_data.drop(columns=['Endpoint Val'])\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "737ad0e5-861e-4cc8-a168-eea1cc7e9f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPipeline:\n",
    "    def __init__(self):\n",
    "        self.endpoint_type = RandomForestClassifier(random_state=42)\n",
    "        self.endpoint_val = RandomForestRegressor(random_state=42)\n",
    "        self.endpoint_units = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    def train_data(self, df):\n",
    "        training_data, testing_data = train_test_split(\n",
    "            df,\n",
    "            test_size=0.25, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        endpoint_encoder = LabelEncoder()\n",
    "        units_encoder = LabelEncoder()\n",
    "        \n",
    "        training_data['endpoint_type_encoded'] = endpoint_encoder.fit_transform(training_data['Endpoint Description'])\n",
    "        training_data['units_encoded'] = units_encoder.fit_transform(training_data['Endpoint Units'])\n",
    "\n",
    "        training_technique_cols = [col for col in training_data.columns if col.startswith('technique_')]\n",
    "        training_stage_cols = [col for col in training_data.columns if col.startswith('stage_')]\n",
    "        training_class_cols = [col for col in training_data.columns if col.startswith('class_')]\n",
    "\n",
    "        model1_features = ['Tox Exposure Duration'] + training_technique_cols\n",
    "        model2_features = training_stage_cols + training_class_cols + training_technique_cols + ['MolecularWeight', 'XLogP', 'TPSA', 'HBondDonorCount', 'HBondAcceptorCount', 'Lipinski_Violations', 'Total_HBonds'] + ['Tox Exposure Duration', 'endpoint_type_encoded']\n",
    "        model3_features = ['endpoint_type_encoded'] + training_technique_cols + training_class_cols\n",
    "\n",
    "\n",
    "        testing_data['endpoint_type_encoded'] = endpoint_encoder.transform(testing_data['Endpoint Description'])\n",
    "        testing_data['units_encoded'] = units_encoder.transform(testing_data['Endpoint Units'])\n",
    "\n",
    "        #Model 1 training\n",
    "        model1_training_data = training_data.copy()\n",
    "        model1_testing_data = testing_data.copy()\n",
    "        \n",
    "        X1_train = model1_training_data[model1_features]\n",
    "        y1_train = model1_training_data['endpoint_type_encoded']\n",
    "        X1_test = model1_testing_data[model1_features]\n",
    "        y1_test = model1_testing_data['endpoint_type_encoded']\n",
    "        \n",
    "        # Train model (no scaling needed for Random Forest)\n",
    "        self.endpoint_type.fit(X1_train, y1_train)\n",
    "        y1_pred = self.endpoint_type.predict(X1_test)\n",
    "        model1_accuracy = accuracy_score(y1_test, y1_pred)\n",
    "\n",
    "        print(f\"Model 1 Accuracy: {model1_accuracy:.3f}\")\n",
    "        print(f\"Evaluated on ALL {len(y1_test)} test samples\")\n",
    "        \n",
    "        # Show classification report for available classes\n",
    "        try:\n",
    "            print(\"Classification Report:\")\n",
    "            print(classification_report(y1_test, y1_pred, target_names=endpoint_encoder.classes_))\n",
    "        except:\n",
    "            print(\"⚠️  Some classes may be missing from test set\")\n",
    "\n",
    "\n",
    "\n",
    "        #Model 2 training\n",
    "        model2_training_data = training_data.copy()\n",
    "        model2_testing_data = testing_data.copy()\n",
    "\n",
    "        X2_train = model2_training_data[model2_features]\n",
    "        y2_train = model2_training_data['log_endpoint_val']\n",
    "        X2_test = model2_testing_data[model2_features]\n",
    "        y2_test = model2_testing_data['log_endpoint_val']\n",
    "\n",
    "        self.endpoint_val.fit(X2_train, y2_train)\n",
    "        y2_pred = self.endpoint_val.predict(X2_test)\n",
    "        model2_r2 = r2_score(y2_test, y2_pred)\n",
    "        model2_rmse = np.sqrt(mean_squared_error(y2_test, y2_pred))\n",
    "        print(f\"Testing R2 score: {model2_r2:.2f}\" )\n",
    "        print(f\"Testing RSME score: {model2_rmse:.2f}\" )\n",
    "        \n",
    "        #Model 3 training\n",
    "        model3_training_data = training_data.copy()\n",
    "        model3_testing_data = testing_data.copy()\n",
    "\n",
    "        X3_train = model3_training_data[model3_features]\n",
    "        y3_train = model3_training_data['units_encoded']\n",
    "        X3_test = model3_testing_data[model3_features]\n",
    "        y3_test = model3_testing_data['units_encoded']\n",
    "\n",
    "        self.endpoint_units.fit(X3_train, y3_train)\n",
    "        y3_pred = self.endpoint_units.predict(X3_test)\n",
    "        model3_accuracy = accuracy_score(y3_test, y3_pred)\n",
    "\n",
    "        print(f\"Model 3 Accuracy: {model3_accuracy:.3f}\")\n",
    "        print(f\"Evaluated on ALL {len(y3_test)} test samples\")\n",
    "\n",
    "        # Show classification report for available classes\n",
    "        try:\n",
    "            print(\"Classification Report:\")\n",
    "            print(classification_report(y3_test, y3_pred, target_names=units_encoder.classes_))\n",
    "        except:\n",
    "            print(\"⚠️  Some classes may be missing from test set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "41122c8c-027f-4da2-b4b8-989acbb1eeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Accuracy: 0.971\n",
      "Evaluated on ALL 34 test samples\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        LC50       0.93      1.00      0.96        13\n",
      "        LD50       1.00      0.95      0.98        21\n",
      "\n",
      "    accuracy                           0.97        34\n",
      "   macro avg       0.96      0.98      0.97        34\n",
      "weighted avg       0.97      0.97      0.97        34\n",
      "\n",
      "Testing R2 score: 0.74\n",
      "Testing RSME score: 1.84\n",
      "Model 3 Accuracy: 1.000\n",
      "Evaluated on ALL 34 test samples\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       mg/kg       1.00      1.00      1.00        26\n",
      "        mg/l       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        34\n",
      "   macro avg       1.00      1.00      1.00        34\n",
      "weighted avg       1.00      1.00      1.00        34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    cleaned_data = clean_data()\n",
    "    model = ModelPipeline()\n",
    "    model.train_data(cleaned_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
