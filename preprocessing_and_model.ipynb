{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7567275-b6ab-4da8-a3e1-6bc3d858ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, QuantileTransformer\n",
    "from sklearn.metrics import classification_report, accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import re\n",
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "975ce2ea-34c9-479f-8535-301957cb05f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(input_file='Data/ToxicityPhylogeneticChemical.csv'):    \n",
    "    all_data = pd.read_csv(input_file)\n",
    "    \n",
    "    # Function to extract just the number\n",
    "    def extract_number(endpoint_str):\n",
    "        if pd.isna(endpoint_str):\n",
    "            return None\n",
    "        match = re.search(r'(\\d+\\.?\\d*)', str(endpoint_str))\n",
    "        return float(match.group(1)) if match else None\n",
    "\n",
    "    # Function to extract units (mg/l or mg/kg)\n",
    "    def extract_units(endpoint_str):\n",
    "        if pd.isna(endpoint_str):\n",
    "            return None\n",
    "        endpoint_str = str(endpoint_str).lower()\n",
    "        if 'mg/l' in endpoint_str:\n",
    "            return 'mg/l'\n",
    "        elif 'mg/kg' in endpoint_str:\n",
    "            return 'mg/kg'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Apply the functions\n",
    "    all_data['Endpoint Val'] = all_data['Endpoint Value'].apply(extract_number)\n",
    "    all_data['Endpoint Units'] = all_data['Endpoint Value'].apply(extract_units)\n",
    "    all_data = all_data.drop(columns=['Endpoint Value'])\n",
    "\n",
    "    # Filter out invalid data\n",
    "    initial_count = len(all_data)\n",
    "    all_data = all_data[all_data['Endpoint Val'] > 0.005]\n",
    "    all_data = all_data.dropna(subset=['Endpoint Description', 'class', 'order'])\n",
    "\n",
    "    # IMPROVED OUTLIER HANDLING: Use 95th percentile instead of fixed cap\n",
    "    endpoint_95th = all_data['Endpoint Val'].quantile(0.95)\n",
    "    all_data['Endpoint Val'] = all_data['Endpoint Val'].clip(lower=0.01, upper=endpoint_95th)\n",
    "\n",
    "    # Hierarchical encoding of phylogenetic data\n",
    "    class_features = pd.get_dummies(all_data['class'], prefix='class')\n",
    "    order_features = pd.get_dummies(all_data['order'], prefix='order')    \n",
    "    phylogenetic_features = pd.concat([class_features, order_features], axis=1)\n",
    "    all_data = pd.concat([all_data, phylogenetic_features], axis=1)\n",
    "        \n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = ['Unnamed: 0', 'Common Name', 'Range', 'Sex', 'Chemical', \n",
    "                      'ChemicalName', 'Sample Size', 'Tox Exposure', 'species', \n",
    "                      'class', 'order', 'family', 'genus']\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in all_data.columns]\n",
    "    all_data = all_data.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Impute XLogP using correlation with molecular weight\n",
    "    if all_data['XLogP'].isna().sum() > 0:\n",
    "        # Simple imputation with median for now\n",
    "        xlogp_median = all_data['XLogP'].median()\n",
    "        all_data['XLogP'] = all_data['XLogP'].fillna(xlogp_median)\n",
    "    \n",
    "    # Handling Tox Exposure Duration\n",
    "    duration_map = ['Single dose','single', 'single dose', 'Single exposure']\n",
    "    all_data['Tox Exposure Duration'] = all_data['Tox Exposure Duration'].replace(duration_map, 0.1)\n",
    "    all_data['Tox Exposure Duration'] = all_data['Tox Exposure Duration'].fillna(0.1)\n",
    "    all_data['Tox Exposure Duration'] = all_data['Tox Exposure Duration'].astype('float64')\n",
    "        \n",
    "    # Handle technique missing values\n",
    "    main_techniques = ['diet', 'waterborne', 'oral']\n",
    "    all_data['Tox Exposure Technique'] = all_data['Tox Exposure Technique'].apply(\n",
    "        lambda x: x if x in main_techniques else 'other'\n",
    "    )\n",
    "\n",
    "    # IMPROVED CHEMICAL FEATURES\n",
    "    all_data['Lipinski_Violations'] = (\n",
    "        (all_data['MolecularWeight'] > 500).astype(int) +\n",
    "        (all_data['XLogP'] > 5).astype(int) +\n",
    "        (all_data['HBondDonorCount'] > 5).astype(int) +\n",
    "        (all_data['HBondAcceptorCount'] > 10).astype(int)\n",
    "    )\n",
    "    all_data['Total_HBonds'] = all_data['HBondDonorCount'] + all_data['HBondAcceptorCount']\n",
    "    \n",
    "    # Add interaction features (only most important ones)\n",
    "    all_data['MW_XLogP'] = all_data['MolecularWeight'] * all_data['XLogP']\n",
    "    all_data['TPSA_HBonds'] = all_data['TPSA'] * all_data['Total_HBonds']\n",
    "    \n",
    "    # IMPROVED FEATURE SCALING\n",
    "    scaler = StandardScaler()\n",
    "    molecular_features = ['MolecularWeight', 'XLogP', 'TPSA', 'MW_XLogP', 'TPSA_HBonds']\n",
    "    all_data[molecular_features] = scaler.fit_transform(all_data[molecular_features])\n",
    "\n",
    "    # Create dummy variables\n",
    "    technique_features = pd.get_dummies(all_data['Tox Exposure Technique'], prefix='technique')\n",
    "    stage_features = pd.get_dummies(all_data['Life Cycle Stage'], prefix='stage')\n",
    "    \n",
    "    all_data = pd.concat([all_data, technique_features, stage_features], axis=1)\n",
    "    all_data = all_data.drop(columns=['Tox Exposure Technique', 'Life Cycle Stage'])\n",
    "\n",
    "    # IMPROVED TARGET TRANSFORMATION\n",
    "    all_data['log_endpoint_val'] = np.log(all_data['Endpoint Val'])\n",
    "    return all_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5b88919-669b-4cd1-898b-5ce8139ec530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeModelPipeline:\n",
    "    def __init__(self):\n",
    "        self.endpoint_type = RandomForestClassifier(random_state=42)\n",
    "\n",
    "        self.endpoint_val = MultiOutputRegressor(\n",
    "            GradientBoostingRegressor(       \n",
    "                random_state=42\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.endpoint_units = RandomForestClassifier(random_state=42)\n",
    "        \n",
    "                # Store components\n",
    "        self.endpoint_encoder = None\n",
    "        self.units_encoder = None\n",
    "        self.scaler = None\n",
    "        self.feature_columns = {\n",
    "            'model1': [],\n",
    "            'model2': [],\n",
    "            'model3': []\n",
    "        }  # Store feature column names for each model\n",
    "\n",
    "    def train_data(self, df, scaler):\n",
    "        self.scaler = scaler\n",
    "        \n",
    "        # Create train/test split\n",
    "        training_data, testing_data = train_test_split(\n",
    "            df, test_size=0.25, random_state=42, stratify=df['Endpoint Description']\n",
    "        )\n",
    "        \n",
    "        print(f\"Training samples: {len(training_data)}\")\n",
    "        print(f\"Testing samples: {len(testing_data)}\")\n",
    "        \n",
    "        self.endpoint_encoder = LabelEncoder()\n",
    "        self.units_encoder = LabelEncoder()\n",
    "        \n",
    "        training_data = training_data.copy()\n",
    "        testing_data = testing_data.copy()\n",
    "        \n",
    "        training_data['endpoint_type_encoded'] = self.endpoint_encoder.fit_transform(training_data['Endpoint Description'])\n",
    "        training_data['units_encoded'] = self.units_encoder.fit_transform(training_data['Endpoint Units'])\n",
    "\n",
    "        # Feature selection\n",
    "        technique_cols = [col for col in training_data.columns if col.startswith('technique_')]\n",
    "        stage_cols = [col for col in training_data.columns if col.startswith('stage_')]\n",
    "        class_cols = [col for col in training_data.columns if col.startswith('class_')]\n",
    "        order_cols = [col for col in training_data.columns if col.startswith('order_')]\n",
    "\n",
    "        model1_features = technique_cols  \n",
    "        model2_features = (stage_cols + class_cols + order_cols + technique_cols + \n",
    "                          ['MolecularWeight', 'XLogP', 'TPSA', 'HBondDonorCount', 'HBondAcceptorCount', \n",
    "                           'Lipinski_Violations', 'Total_HBonds', 'MW_XLogP', 'TPSA_HBonds'] + \n",
    "                          ['Tox Exposure Duration', 'endpoint_type_encoded'])\n",
    "        model3_features = ['endpoint_type_encoded'] + technique_cols + class_cols\n",
    "\n",
    "        self.feature_columns['model1'] = model1_features\n",
    "        self.feature_columns['model2'] = model2_features\n",
    "        self.feature_columns['model3'] = model3_features\n",
    "\n",
    "\n",
    "        testing_data['endpoint_type_encoded'] = self.endpoint_encoder.transform(testing_data['Endpoint Description'])\n",
    "        testing_data['units_encoded'] = self.units_encoder.transform(testing_data['Endpoint Units'])\n",
    "\n",
    "        print(\"\\n--- MODEL 1: Endpoint Type Classification ---\")\n",
    "        X1_train = training_data[model1_features]\n",
    "        y1_train = training_data['endpoint_type_encoded']\n",
    "        X1_test = testing_data[model1_features]\n",
    "        y1_test = testing_data['endpoint_type_encoded']\n",
    "        \n",
    "        self.endpoint_type.fit(X1_train, y1_train)\n",
    "        y1_pred = self.endpoint_type.predict(X1_test)\n",
    "        model1_accuracy = accuracy_score(y1_test, y1_pred)\n",
    "\n",
    "        print(f\"Accuracy: {model1_accuracy:.3f}\")\n",
    "        print(classification_report(y1_test, y1_pred, target_names=self.endpoint_encoder.classes_))\n",
    "\n",
    "        # MODEL 2\n",
    "        print(\"\\n--- MODEL 2: Endpoint Value Regression ---\")\n",
    "        X2_train = training_data[model2_features]\n",
    "        X2_test = testing_data[model2_features]\n",
    "\n",
    "        # Multi-target training\n",
    "        y2_train = np.column_stack([\n",
    "            training_data['log_endpoint_val'],\n",
    "            training_data['Endpoint Val']\n",
    "        ])\n",
    "        \n",
    "        y2_test = np.column_stack([\n",
    "            testing_data['log_endpoint_val'],\n",
    "            testing_data['Endpoint Val']\n",
    "        ])\n",
    "\n",
    "        self.endpoint_val.fit(X2_train, y2_train)\n",
    "        y2_pred = self.endpoint_val.predict(X2_test)\n",
    "        \n",
    "        log_pred = y2_pred[:, 0]\n",
    "        original_pred = y2_pred[:, 1]\n",
    "        \n",
    "        r2_log = r2_score(y2_test[:, 0], log_pred)\n",
    "        r2_original_direct = r2_score(y2_test[:, 1], original_pred)\n",
    "        \n",
    "        print(f\"Log space R²: {r2_log:.3f}\")\n",
    "        print(f\"Direct original R²: {r2_original_direct:.3f}\")\n",
    "\n",
    "        # MODEL 3\n",
    "        print(\"\\n--- MODEL 2: Endpoint Units Classification ---\")\n",
    "        X3_train = training_data[model3_features]\n",
    "        y3_train = training_data['units_encoded']\n",
    "        X3_test = testing_data[model3_features]\n",
    "        y3_test = testing_data['units_encoded']\n",
    "\n",
    "        self.endpoint_units.fit(X3_train, y3_train)\n",
    "        y3_pred = self.endpoint_units.predict(X3_test)\n",
    "        model3_accuracy = accuracy_score(y3_test, y3_pred)\n",
    "        print(f\"Accuracy: {model3_accuracy:.3f}\")\n",
    "        print(classification_report(y3_test, y3_pred, target_names=self.units_encoder.classes_))\n",
    "\n",
    "    def save_models(self, model_dir='saved_models'):\n",
    "        \"\"\"Save all models and components to disk\"\"\"\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save individual models using joblib (better for sklearn models)\n",
    "        joblib.dump(self.endpoint_type, os.path.join(model_dir, 'endpoint_type_model.pkl'))\n",
    "        joblib.dump(self.endpoint_val, os.path.join(model_dir, 'endpoint_val_model.pkl'))\n",
    "        joblib.dump(self.endpoint_units, os.path.join(model_dir, 'endpoint_units_model.pkl'))\n",
    "        \n",
    "        # Save encoders and scaler\n",
    "        joblib.dump(self.endpoint_encoder, os.path.join(model_dir, 'endpoint_encoder.pkl'))\n",
    "        joblib.dump(self.units_encoder, os.path.join(model_dir, 'units_encoder.pkl'))\n",
    "        joblib.dump(self.scaler, os.path.join(model_dir, 'scaler.pkl'))\n",
    "        \n",
    "        # Save metadata (feature columns, training columns)\n",
    "        metadata = {\n",
    "            'feature_columns': self.feature_columns,\n",
    "            'endpoint_classes': self.endpoint_encoder.classes_.tolist() if self.endpoint_encoder else None,\n",
    "            'units_classes': self.units_encoder.classes_.tolist() if self.units_encoder else None,\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(model_dir, 'model_metadata.pkl'), 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        print(f\"All models and components saved to {model_dir}/\")\n",
    "        print(\"Files saved:\")\n",
    "        for file in os.listdir(model_dir):\n",
    "            print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c6cb3f2-c7a5-483d-991d-9af7caedf451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 96\n",
      "Testing samples: 32\n",
      "\n",
      "--- MODEL 1: Endpoint Type Classification ---\n",
      "Accuracy: 0.969\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        LC50       0.93      1.00      0.96        13\n",
      "        LD50       1.00      0.95      0.97        19\n",
      "\n",
      "    accuracy                           0.97        32\n",
      "   macro avg       0.96      0.97      0.97        32\n",
      "weighted avg       0.97      0.97      0.97        32\n",
      "\n",
      "\n",
      "--- MODEL 2: Endpoint Value Regression ---\n",
      "Log space R²: 0.710\n",
      "Direct original R²: 0.765\n",
      "\n",
      "--- MODEL 2: Endpoint Units Classification ---\n",
      "Accuracy: 1.000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       mg/kg       1.00      1.00      1.00        24\n",
      "        mg/l       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        32\n",
      "   macro avg       1.00      1.00      1.00        32\n",
      "weighted avg       1.00      1.00      1.00        32\n",
      "\n",
      "All models and components saved to saved_models/\n",
      "Files saved:\n",
      "  - scaler.pkl\n",
      "  - endpoint_type_model.pkl\n",
      "  - endpoint_encoder.pkl\n",
      "  - units_encoder.pkl\n",
      "  - endpoint_units_model.pkl\n",
      "  - endpoint_val_model.pkl\n",
      "  - model_metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    cleaned_data, scaler = clean_data()\n",
    "    model = ThreeModelPipeline()\n",
    "    model.train_data(cleaned_data, scaler)\n",
    "    model.save_models()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
